{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ECG Neural Network Training with Different Activations and Optimizers"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Imports\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.datasets import fetch_openml\n", "\n", "# Load ECG Dataset\n", "ecg_data = fetch_openml(name=\"ECG5000\", version=1, as_frame=False)\n", "X, y = ecg_data['data'], ecg_data['target'].astype(int)\n", "\n", "# Binary classification: class 1 vs others\n", "y = (y == 1).astype(int).reshape(-1, 1)\n", "\n", "# Normalize input features\n", "X = StandardScaler().fit_transform(X)\n", "\n", "# Split into train/test\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Activation Functions\n", "def sigmoid(z): return 1 / (1 + np.exp(-z))\n", "def dsigmoid(a): return a * (1 - a)\n", "\n", "def tanh(z): return np.tanh(z)\n", "def dtanh(a): return 1 - np.power(a, 2)\n", "\n", "def relu(z): return np.maximum(0, z)\n", "def drelu(a): return (a > 0).astype(float)\n", "\n", "def leaky_relu(z, alpha=0.01): return np.where(z > 0, z, z * alpha)\n", "def dleaky_relu(a, alpha=0.01): return np.where(a > 0, 1, alpha)\n", "\n", "def elu(z, alpha=1): return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n", "def delu(a, alpha=1): return np.where(a > 0, 1, a + alpha)\n", "\n", "def prelu(z, alpha=0.25): return np.where(z > 0, z, alpha * z)\n", "def dprelu(a, alpha=0.25): return np.where(a > 0, 1, alpha)\n", "\n", "def selu(z, alpha=1.67326, scale=1.0507):\n", "    return scale * np.where(z > 0, z, alpha * (np.exp(z) - 1))\n", "def dselu(a, alpha=1.67326, scale=1.0507):\n", "    return np.where(a > 0, scale, scale * alpha * np.exp(a))\n", "\n", "activations = {\n", "    \"sigmoid\": (sigmoid, dsigmoid),\n", "    \"tanh\": (tanh, dtanh),\n", "    \"relu\": (relu, drelu),\n", "    \"leaky_relu\": (leaky_relu, dleaky_relu),\n", "    \"elu\": (elu, delu),\n", "    \"prelu\": (prelu, dprelu),\n", "    \"selu\": (selu, dselu),\n", "}"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Loss Function\n", "def binary_cross_entropy(y_true, y_pred):\n", "    y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)\n", "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n", "\n", "def dbce(y_true, y_pred):\n", "    return (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-9)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Optimizers\n", "def gradient_descent(params, grads, lr):\n", "    return [p - lr * g for p, g in zip(params, grads)]\n", "\n", "def stochastic_gradient_descent(params, grads, lr):\n", "    return [p - lr * g for p, g in zip(params, grads)]\n", "\n", "def mini_batch_sgd(params, grads, lr):\n", "    return [p - lr * g for p, g in zip(params, grads)]"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Neural Network Class\n", "class SimpleANN:\n", "    def __init__(self, input_dim, hidden_dim, activation_name, optimizer_name, lr=0.1):\n", "        self.activation_name = activation_name\n", "        self.optimizer_name = optimizer_name\n", "        self.lr = lr\n", "        self.act, self.dact = activations[activation_name]\n", "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n", "        self.b1 = np.zeros((1, hidden_dim))\n", "        self.W2 = np.random.randn(hidden_dim, 1) * 0.1\n", "        self.b2 = np.zeros((1, 1))\n", "\n", "    def forward(self, X):\n", "        self.Z1 = X @ self.W1 + self.b1\n", "        self.A1 = self.act(self.Z1)\n", "        self.Z2 = self.A1 @ self.W2 + self.b2\n", "        self.A2 = sigmoid(self.Z2)\n", "        return self.A2\n", "\n", "    def backward(self, X, y, output):\n", "        dA2 = dbce(y, output)\n", "        dZ2 = dA2 * dsigmoid(output)\n", "        dW2 = self.A1.T @ dZ2\n", "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n", "        dA1 = dZ2 @ self.W2.T\n", "        dZ1 = dA1 * self.dact(self.A1)\n", "        dW1 = X.T @ dZ1\n", "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n", "        return [dW1, db1, dW2, db2]\n", "\n", "    def update(self, grads):\n", "        params = [self.W1, self.b1, self.W2, self.b2]\n", "        if self.optimizer_name == 'gd':\n", "            updated = gradient_descent(params, grads, self.lr)\n", "        elif self.optimizer_name == 'sgd':\n", "            updated = stochastic_gradient_descent(params, grads, self.lr)\n", "        elif self.optimizer_name == 'mini-batch':\n", "            updated = mini_batch_sgd(params, grads, self.lr)\n", "        self.W1, self.b1, self.W2, self.b2 = updated\n", "\n", "    def train(self, X, y, epochs=100, batch_size=None):\n", "        loss_list = []\n", "        for epoch in range(epochs):\n", "            if batch_size:\n", "                indices = np.random.permutation(len(X))\n", "                X, y = X[indices], y[indices]\n", "                for i in range(0, len(X), batch_size):\n", "                    X_batch = X[i:i+batch_size]\n", "                    y_batch = y[i:i+batch_size]\n", "                    output = self.forward(X_batch)\n", "                    grads = self.backward(X_batch, y_batch, output)\n", "                    self.update(grads)\n", "            else:\n", "                output = self.forward(X)\n", "                grads = self.backward(X, y, output)\n", "                self.update(grads)\n", "            y_pred = self.forward(X)\n", "            loss = binary_cross_entropy(y, y_pred)\n", "            loss_list.append(loss)\n", "        return loss_list\n", "\n", "    def predict(self, X):\n", "        probs = self.forward(X)\n", "        return (probs > 0.5).astype(int)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Run Experiments\n", "combinations = [\n", "    ('sigmoid', 'gd'),\n", "    ('relu', 'gd'),\n", "    ('tanh', 'sgd'),\n", "    ('leaky_relu', 'mini-batch'),\n", "    ('prelu', 'mini-batch'),\n", "    ('elu', 'sgd'),\n", "    ('selu', 'gd')\n", "]\n", "\n", "results = {}\n", "\n", "for act_name, opt_name in combinations:\n", "    print(f\"Training with {act_name.upper()} + {opt_name.upper()}...\")\n", "    model = SimpleANN(input_dim=X.shape[1], hidden_dim=10, activation_name=act_name, optimizer_name=opt_name, lr=0.1)\n", "    batch = 32 if opt_name == 'mini-batch' else None\n", "    loss_curve = model.train(X_train, y_train, epochs=100, batch_size=batch)\n", "    y_pred = model.predict(X_test)\n", "    acc = np.mean(y_pred == y_test)\n", "    results[f\"{act_name}_{opt_name}\"] = (loss_curve, acc)\n", "\n", "# Plot Individual Loss Curves and Global Minimum\n", "for name, (losses, acc) in results.items():\n", "    plt.figure(figsize=(6, 4))\n", "    plt.plot(losses, label=f\"{name} (Acc: {acc:.2f})\")\n", "    plt.scatter(np.argmin(losses), min(losses), color='red', label=f\"Min Loss = {min(losses):.4f}\")\n", "    plt.title(f\"Loss Curve - {name}\")\n", "    plt.xlabel(\"Epoch\")\n", "    plt.ylabel(\"Loss\")\n", "    plt.legend()\n", "    plt.grid(True)\n", "    plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}